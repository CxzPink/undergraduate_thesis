\cleardoublepage

\section{新的图卷积神经网络的设计}
本部分将介绍，将图卷积神经网络等效为低通滤波器[11]的近期研究结果，并以此作为启示详细介绍我们
设计的新图卷积神经网络。
\subsection{低通滤波器}
近期的研究，有学者提出了一个基于图信号处理的分析图神经网络的理论框架。实验结果表明，图神经网络
只对特征向量进行低通滤波，不具有非线性流形学习的特征，他们进一步研究了网络对特征噪声的适应能力，
并对基于GCN[12]的图神经网络设计提出了一些见解。
\subsubsection{重要假设}

    \textbf{假设1} \quad 输入的特征向量包含低频的真实信号和噪声，真实的特征量为机器学习提供充分的信息。
    
    这篇文章的作者，在常用的数据集上验证了假设1，实验结果（图3.1）显示了在具有不同数量频率成分的
    特征向量上训练的两层感知器（MLP）的性能。在所有基准数据中，我们看到只有少量的频率成分有助于学习。当加入一
    些高斯噪声，在特征向量中加入更多的频率分量只会降低性能。
    \begin{figure}[ht]
        \centering
        \captionsetup{width=10cm}
        \includegraphics[width=12cm]{design/1.jpg}
        \caption{\label{3-1}对于Cora、citeseer、pubmed数据集，两层感知器（MLP）在不同频率的特征向量上的性能和gfNN网络最佳性能的对比}
    \end{figure}
    
    \textbf{假设2} \quad 
    观测到的特征信号${x(i)}_{i \in \nu}$包含了真实的信号${\bar{x}(i)}_{i \in \nu}$和噪声${z(i)}_{i \in \nu}$.
    真实信号$\bar{X}$的频率大多在$ 0 \le \epsilon \ll 1 $，噪声按照高斯白噪声分布，噪声$Z$的傅里叶变换的每一项都独立地
    服从正太分布$ N(0,\sigma^{2} ) $。


\subsubsection{理论推导}
不妨认为$ \hat{A}X $可以较精确地近似$ \bar{X} $，接下来理论推导图卷积神经网络的效果，具体细节可见[11]。
我们可以将两层感知器（MLP）表示为：
$$ h_{MLP}(X|W_1,W_2) = \sigma_{2}(\sigma_{1}(XW_1)W_2) $$
其中$\sigma_{1}$代表ReLU函数，$\sigma_{2}$代表softmax函数。$\sigma_{1}$和$\sigma_{2}$都是一种收敛的映射，
即$  \left \| \sigma_{i}（X）- \sigma_{i}（Y） \right \|_{D}  \le  \left \| X-Y \right \|_{D} $。

在假设1的前提下，我们的目标是获得与$ h_{MLP}(\bar{X}|W_1,W_2) $相类似的结果。最简单的方法是用观测到的特征信号，
来训练两层感知器（MLP）$ h_{MLP}(X|W_1,W_2) $。这种方法的效果可以做以下估计：
$$  \left \| h_{MLP}(\bar{X}|W_1,W_2) - h_{MLP}(X|W_1,W_2) \right \|_{D}  \le  \left \| Z \right \|_{D} \rho(W_{1}) \rho(W_{2}) $$
其中$ \rho(W) $是$W$的最大特征值。

现在，我们可以先使用图滤波器$ \hat{A}X $来估计特征信号，然后再用感知器$ h_{MLP}(\hat{A}X|W_1,W_2) $来训练,
通过推导我们可以得到以下结果：
$$  \left \| h_{MLP}(\bar{X}|W_1,W_2) - h_{MLP}(\hat{A}X|W_1,W_2) \right \|_{D}  =  \tilde{0}(\sqrt[]{\epsilon }) E[\left \| Z \right \|_{D}] \rho(W_{1}) \rho(W_{2}) $$

这意味着，如果真实数据的最大频率很小，我们可以这种方法得到一个近似于最优的解决方案。

\subsubsection{阶段性结论}

\begin{itemize}
    \item \textbf{结论1} \quad
    证明了将图信号与传播矩阵相乘，对应于图信号通过低通滤波。此外，还证明了观测信号与低通滤波器之间的
    矩阵乘积，实际上是求真实信号估计问题的解析解。从图信号处理理论的角度，我们的结果表明，
    图卷积层的设计，某种程度上就是低通滤波器的设计。
    
    \item \textbf{结论2} \quad
    基于这种理论理解，它提出了一个新的图过滤神经网络框架gfNN来处理顶点分类问题。
    gfNN网络包括两个步骤:1.通过与图过滤矩阵相乘来过滤特征；2.通过机器学习模型学习顶点标签。
    gfNN神经网络结构与传统GCN神经网络结构对比，如图3.2所示。
    \begin{figure}[ht]
        \centering
        \captionsetup{width=10cm}
        \includegraphics[width=12cm]{design/2.jpg}
        \caption{\label{3-2}GCN、SGC、gfNN 三种图卷积神经网络结构对比}
    \end{figure}
\end{itemize}

\subsection{新的设计方法}
由于效率、通用性和灵活性问题，空间模型在大多数场合比频谱模型更受欢迎。注意力机制图神经网络（GAT）
是一种最为通用的空间模型，网络结构如图3.3所示，我们将基于GAT来提出我们的新的图卷积神经网络（our GAT）。

我们在第二章理论基础部分，曾提及GAT神经网络的特点，现再对其主要缺点做简单总结。首先，是注意力
矩阵$A$必须是1-hop的，如果直接将其变成2-hop将出现严重的过拟合现象；其次，我们观察图3.3，可以发现GAT有两层
神经网络，这使得GAT需要较长的时间来训练网络，然而直接将GAT变成一层的神经网络，它的表达能力又将会
大打折扣。

为了解决上述两个问题，我们融合了频谱理论，提出了新的图卷积神经网络（our GAT），如图3.4所示。
接下去，我们将详细介绍our GAT的神经网络结构。
\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \captionsetup{width=5cm}
    \includegraphics[width=6cm]{design/3.jpg}
    \caption{\label{3-3}GAT}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \captionsetup{width=5cm}
    \includegraphics[width=6cm]{design/4.jpg}
    \caption{\label{3-4}our GAT}
    \end{minipage}
\end{figure}

\subsubsection{第一步}
\begin{figure}[ht]
    \centering
    \captionsetup{width=10cm}
    \includegraphics[width=12cm]{design/5.jpg}
    \caption{\label{3-5}第一步}
\end{figure}
第一步，如图3.5所示，输入的特征信号$ X \in R^{F*n} $，通过待训练的权重矩阵$ W_1 \in R^{F'*F} $，输出的
信号为$ W_{1}X \in R^{F'*n} $。其作用主要是给输入信号降维和线性变换。因为每个节点上的输入信号有时会有很高
的维度，如在cora数据集中$ F = 1433 $，过高的特征维度势必会给后续的滤波和机器学习带来挑战，所以这一步我们使用
权重矩阵$ W_1，\in R^{F'*F}，F \ll F $来对输入信号进行压缩和线性变换。

\subsubsection{第二步}
\begin{figure}[ht]
    \centering
    \captionsetup{width=10cm}
    \includegraphics[width=12cm]{design/6.jpg}
    \caption{\label{3-6}第二步}
\end{figure}
第二步，如图3.6所示，输入的信号为$ W_{1}X \in R^{F'*n} $，利用注意力参数$ \vec{a} \in R^{2F'} $，得到注意力矩阵
$A \in R^{n*n}$。这一步的实现和GAT类似，主要作用是训练得到边权重（或称之为注意力权重）。注意力权重衡量节点$i$和
它相邻节点$j$的连接权重，对于不相邻的节点，$A_{ij} = 0$，对于相邻的节点，$ A_{ij} = \frac{exp(LeakyReLU(\vec{a}^{T}[W_{1}\vec{x_{i}}||W_{1}\vec{x_{j}}]))} { {\textstyle \sum_{k\in N_{i} }^{}} exp(LeakyReLU(\vec{a}^{T}[W_{1}\vec{x_{i}}||W_{1}\vec{x_{k}}]))} $。
相较于直接学习每条边的权重，借助注意力参数$ \vec{a} \in R^{2F'} $间接的生成边权重，减少了参数量，有效地避免了过拟合。

\subsubsection{第三步}
\begin{figure}[ht]
    \centering
    \captionsetup{width=10cm}
    \includegraphics[width=12cm]{design/7.jpg}
    \caption{\label{3-7}第三步}
\end{figure}
第三步，如图3.7所示，输入为注意力矩阵$A \in R^{n*n}$，通过矩阵多项式$ {\sum_{i=1}^{K}c_{i}A^{i}} $ ，得到新的矩阵
$F(A) \in R^{n*n}$。这一步，是我们设计的图卷积神经网络的主要创新，将频谱理论中使用矩阵多项式的设计方法应用到GAT中来。
我们在第二章理论基础中提到，GAT的图结构必须是1-hop，矩阵多项式使得我们自由选择图的hop。这是因为虽然注意力矩阵$A$的hop=1，
但是它是自连的，所以它的矩阵多项式$ {\sum_{i=1}^{K}c_{i}A^{i}} $的hop值为$K$，这表明了不仅仅图上的相邻节点有连接权重，
间接相连的节点同样有连接权重，这将大大增强图卷积神经网络的表达能力。

\subsubsection{第四步}
\begin{figure}[ht]
    \centering
    \captionsetup{width=10cm}
    \includegraphics[width=12cm]{design/8.jpg}
    \caption{\label{3-8}第四步}
\end{figure}
第四步，如图3.8所示，输入为经过滤波器$ F(A) $后的信号$ F(A)W_{1}X $，通过待训练的全连接层$ W_2 \in R^{F''*F'} $，
得到输出信号为$ W_{2}F(A)W_{1}X \in R^{F''*n} $。其作用是得到我们期望的节点分类的结果。与卷积神经网络(CNN)类似，
全连接层在整个网络中起到“分类器”的功能，将学到“分布式特征表示”映射到样本标记空间。即在图信号特征提取（滤波）之后，
我们需要通过全连接层得到我们期待的节点分类的结果。

\subsection{设计思路总结}
总的来说，我们希望结合频谱模型和空间模型的设计方法，得到兼有其优点的图卷积神经网络。

\begin{itemize}
    \item \textbf{思路一} \quad
    将注意力机制应用于图卷积神经网络。不仅仅在图神经网络，注意力机制几乎在所有类型的神经网络都取得巨大的成功，简单
    的学习机制、较少的参数量、强大的表达能力，使得注意力机制成为了我们在设计复杂网络过程时的必然选择。此外，将注意力
    机制应用在图卷积神经网络的效果，在GAT中已经有了清楚体现，所以我们选择注意力机制来设计我们的滤波器。
    
    \item \textbf{思路二} \quad
    将矩阵多项式应用于图卷积神经网络。在频谱理论中，因为矩阵都是稀疏的，所以矩阵多项式能够使得图滤波器在很高的训练
    效率下，获得出色的拟合能力。此外，矩阵多项式还在无形中拓宽了图结构的hop，所以如果能将矩阵多项式应用图滤波器的
    设计，将在较小的训练代价下，使得神经网络的表达能力获得提高。

    \item \textbf{思路三} \quad
    相对于GAT网络，减少图滤波器的数量。在本章第一节，我们得到结论图卷积神经网络的效果类似于图低通滤波器，且过多的低通滤波
    器不仅对滤波效果增强不大，还容易造成过拟合现象，所以gfNN网络仅用了一个低通滤波器就达到了理想的效果。我们在图3.3中可见，
    GAT使用了$A_{1}$和$A_{2}$两个图滤波器，这对训练效率有很大的影响。所以如果图滤波器的数量存在冗余，我们能否更加合理地设计
    图卷积神经网络，进而提高训练效率。

\end{itemize}


